{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa071b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class GPTDataset():\n",
    "    def __init__(self, model_path, file_path):\n",
    "        self.model_path = model_path\n",
    "        self.file_path = file_path\n",
    "        self.sp = self.load_tokenizer()\n",
    "        self.train_data, self.val_data = self.load_data()\n",
    "\n",
    "    # 加载tokenizer\n",
    "    def load_tokenizer(self):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load(self.model_path)\n",
    "        return sp\n",
    "\n",
    "    # 加载数据\n",
    "    def load_data(self, split_rate=0.8):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        train_data = text[:int(len(text) * split_rate)]\n",
    "        val_data = text[int(len(text) * split_rate):]\n",
    "        sp = self.load_tokenizer()\n",
    "        train_data = sp.Encode(train_data, out_type=int)\n",
    "        val_data = sp.Encode(val_data, out_type=int)\n",
    "        return train_data, val_data\n",
    "\n",
    "    # 生成数据迭代器\n",
    "    def seq_data(self, data=\"train_data\", win_len=128, batch_size=10):\n",
    "        if data == \"train_data\":\n",
    "            data = self.train_data\n",
    "        elif data == \"val_data\":\n",
    "            data = self.val_data\n",
    "        data = data[random.randint(0, win_len-1):]\n",
    "        num_subseqs = (len(data)-1)//win_len\n",
    "        num_indexs = list(range(0, num_subseqs*win_len, win_len))\n",
    "        random.shuffle(num_indexs)\n",
    "        #生成迭代器\n",
    "        num_batch = len(num_indexs)//batch_size\n",
    "        for i in range(0, num_batch*batch_size, batch_size):\n",
    "            x = [data[j:j+win_len] for j in num_indexs[i:i+batch_size]]\n",
    "            y = [data[j+1:j+win_len+1] for j in num_indexs[i:i+batch_size]]\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef8250a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class GPTconfig:\n",
    "    vocab_size: int = 20000  # 词表大小\n",
    "    seq_len: int = 128  # 序列长度,即模型将接收和处理的每个样本中的单词数量\n",
    "    embed_dim: int = 128  # 嵌入维度\n",
    "    n_head: int = 4  # 注意力头数\n",
    "    n_layer: int = 4  # Transformer层数\n",
    "    dropout: float = 0.0\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(self.config.dropout)\n",
    "        self.P = torch.zeros((1, config.seq_len, config.embed_dim)).to(device = self.config.device)\n",
    "        X = torch.arange(config.seq_len, dtype=torch.float).reshape(-1, 1)/\\\n",
    "            torch.pow(10000, torch.arange(0, config.embed_dim, 2, dtype=torch.float)/config.embed_dim)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x: [batch_size, seq_len, embed_dim]\n",
    "        x = x + self.P[:, :x.shape[1], :].requires_grad_(False) #位置编码不求梯度\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"层归一化\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.LayerNorm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.LayerNorm(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"自注意力机制\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def mask_softmax(self, attn):\n",
    "        \"\"\"掩码softmax\"\"\"\n",
    "        seq_len = self.config.seq_len\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len))\\\n",
    "            .view(1, seq_len, seq_len).requires_grad_(False)#创建一个下三角矩阵,不更新梯度\n",
    "        mask = mask.to(device = self.config.device)\n",
    "        attn = attn.masked_fill(mask[:, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        return attn\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))/math.sqrt(self.config.seq_len)#transpose() 函数在 PyTorch 中用于交换张量的两个维度        attn = attn\n",
    "        attn = self.mask_softmax(attn)\n",
    "        attn = torch.bmm(self.attn_dropout(attn), v)\n",
    "        return attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头自注意力机制\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wk = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.wq = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.wv = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "        self.selfattention = SelfAttention(config)\n",
    "        self.wo = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def transpose_qkv(self, qkv):\n",
    "        \"\"\"将qkv的维度进行转换，分出n_head个头\"\"\"\n",
    "        qkv = qkv.reshape(qkv.shape[0], qkv.shape[1], self.config.n_head, -1) #[batch_size, seq_len, embed_dim]->[batch_size, seq_len, n_head, embed_dim/n_head]\n",
    "        qkv = qkv.permute(0, 2, 1, 3) #[batch_size, n_head, seq_len, embed_dim/n_head]\n",
    "        qkv = qkv.reshape(-1, qkv.shape[2], qkv.shape[3]) #[batch_size*n_head, seq_len, embed_dim/n_head]\n",
    "        return qkv\n",
    "\n",
    "    def output_cat(self, attn):\n",
    "        \"\"\"将transpose_qkv操作反转\"\"\"\n",
    "        attn = attn.reshape(-1, self.config.n_head, attn.shape[1], attn.shape[2]) #[batch_size*n_head, seq_len, embed_dim/n_head]->[batch_size, n_head, seq_len, embed_dim/n_head]\n",
    "        attn = attn.permute(0, 2, 1, 3) #[batch_size, seq_len, n_head, embed_dim/n_head]\n",
    "        attn = attn.reshape(attn.shape[0], attn.shape[1], -1) #[batch_size, seq_len, embed_dim]\n",
    "        return attn\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.transpose_qkv(self.wk(x))\n",
    "        q = self.transpose_qkv(self.wq(x))\n",
    "        v = self.transpose_qkv(self.wv(x))\n",
    "        attn = self.selfattention(q, k, v) #[batch_size*n_head, seq_len, embed_dim/n_head]\n",
    "        attn = self.output_cat(attn)\n",
    "        return self.wo(attn)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"两个全连接层的mlp\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim, 4 * config.embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.embed_dim, config.embed_dim),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer块\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.ln1 = LayerNorm(config)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln2 = LayerNorm(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attn(self.ln1(x))\n",
    "        out = x + out\n",
    "        out2 = self.ffn(self.ln2(out))\n",
    "        out2 = out + out2\n",
    "        return out2\n",
    "\n",
    "\n",
    "class GPTmodel(nn.Module):\n",
    "    \"\"\"主模型\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.position = PositionalEncoding(config)\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln = LayerNorm(config)\n",
    "        self.fc = nn.Linear(config.embed_dim, config.vocab_size)\n",
    "\n",
    "    def forward(self, features, targets=None):\n",
    "        x = self.position(self.embeding(features))\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        if targets is not None:\n",
    "            x = self.fc(x) #x: [batch_size, seq_len, embed]->[batch_size, seq_len, vocab_size]\n",
    "            x = x.view(-1, x.shape[-1])\n",
    "        else:\n",
    "            x = self.fc(x[:, -1, :])\n",
    "            x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def genarate(self, seq, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            x = seq[:, -self.config.seq_len:] #seq: [batch_size, seq_len]\n",
    "            new_w = self.forward(x) #new_w: [batch_size, vocab_size]\n",
    "            new_token_index = torch.multinomial(new_w, num_samples=1)#从新词概率分布中采样（抽取）一个新词\n",
    "            seq = torch.cat((seq, new_token_index), dim=1)\n",
    "        return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5111ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60 loss: 9.311641502380372 -----------train\n",
      "epoch: 60 val loss: 9.342051347096762 ------val\n",
      "epoch: 120 loss: 9.127431983947753 -----------train\n",
      "epoch: 120 val loss: 9.167149861653646 ------val\n",
      "epoch: 180 loss: 9.02469596862793 -----------train\n",
      "epoch: 180 val loss: 9.073141892751059 ------val\n",
      "epoch: 240 loss: 8.946689682006836 -----------train\n",
      "epoch: 240 val loss: 8.99762757619222 ------val\n",
      "epoch: 300 loss: 8.886507606506347 -----------train\n",
      "epoch: 300 val loss: 8.938309987386068 ------val\n",
      "epoch: 360 loss: 8.835389328002929 -----------train\n",
      "epoch: 360 val loss: 8.892326831817627 ------val\n",
      "epoch: 420 loss: 8.789538345336915 -----------train\n",
      "epoch: 420 val loss: 8.848799546559652 ------val\n",
      "epoch: 480 loss: 8.748546714782714 -----------train\n",
      "epoch: 480 val loss: 8.807405312856039 ------val\n",
      "epoch: 540 loss: 8.71168571472168 -----------train\n",
      "epoch: 540 val loss: 8.773582776387533 ------val\n",
      "epoch: 600 loss: 8.677050704956054 -----------train\n",
      "epoch: 600 val loss: 8.73988151550293 ------val\n",
      "epoch: 660 loss: 8.64604591369629 -----------train\n",
      "epoch: 660 val loss: 8.710188070933023 ------val\n",
      "epoch: 720 loss: 8.619326210021972 -----------train\n",
      "epoch: 720 val loss: 8.683387915293375 ------val\n",
      "epoch: 780 loss: 8.592452850341797 -----------train\n",
      "epoch: 780 val loss: 8.661649862925211 ------val\n",
      "epoch: 840 loss: 8.56921703338623 -----------train\n",
      "epoch: 840 val loss: 8.639521280924479 ------val\n",
      "epoch: 900 loss: 8.54848777770996 -----------train\n",
      "epoch: 900 val loss: 8.618546485900879 ------val\n",
      "epoch: 960 loss: 8.529545516967774 -----------train\n",
      "epoch: 960 val loss: 8.600674152374268 ------val\n",
      "epoch: 1020 loss: 8.510091934204102 -----------train\n",
      "epoch: 1020 val loss: 8.586571216583252 ------val\n",
      "epoch: 1080 loss: 8.494122467041016 -----------train\n",
      "epoch: 1080 val loss: 8.569751580556234 ------val\n",
      "epoch: 1140 loss: 8.477914962768555 -----------train\n",
      "epoch: 1140 val loss: 8.557652791341146 ------val\n",
      "epoch: 1200 loss: 8.462334213256836 -----------train\n",
      "epoch: 1200 val loss: 8.5421994527181 ------val\n",
      "epoch: 1260 loss: 8.448849067687988 -----------train\n",
      "epoch: 1260 val loss: 8.529760201772055 ------val\n",
      "epoch: 1320 loss: 8.435528564453126 -----------train\n",
      "epoch: 1320 val loss: 8.516327381134033 ------val\n",
      "epoch: 1380 loss: 8.42457088470459 -----------train\n",
      "epoch: 1380 val loss: 8.504862626393637 ------val\n",
      "epoch: 1440 loss: 8.411473083496094 -----------train\n",
      "epoch: 1440 val loss: 8.497691631317139 ------val\n",
      "epoch: 1500 loss: 8.39961898803711 -----------train\n",
      "epoch: 1500 val loss: 8.483415921529135 ------val\n",
      "epoch: 1560 loss: 8.388711814880372 -----------train\n",
      "epoch: 1560 val loss: 8.476099809010824 ------val\n",
      "epoch: 1620 loss: 8.378304023742675 -----------train\n",
      "epoch: 1620 val loss: 8.467951456705729 ------val\n",
      "epoch: 1680 loss: 8.368345680236816 -----------train\n",
      "epoch: 1680 val loss: 8.460379282633463 ------val\n",
      "epoch: 1740 loss: 8.35946403503418 -----------train\n",
      "epoch: 1740 val loss: 8.448790550231934 ------val\n",
      "epoch: 1800 loss: 8.35029941558838 -----------train\n",
      "epoch: 1800 val loss: 8.440858999888102 ------val\n",
      "epoch: 1860 loss: 8.341746788024903 -----------train\n",
      "epoch: 1860 val loss: 8.435451825459799 ------val\n",
      "epoch: 1920 loss: 8.333290557861329 -----------train\n",
      "epoch: 1920 val loss: 8.429623444875082 ------val\n",
      "epoch: 1980 loss: 8.325242080688476 -----------train\n",
      "epoch: 1980 val loss: 8.419683615366617 ------val\n",
      "epoch: 2040 loss: 8.3179940032959 -----------train\n",
      "epoch: 2040 val loss: 8.413410186767578 ------val\n",
      "epoch: 2100 loss: 8.309451675415039 -----------train\n",
      "epoch: 2100 val loss: 8.409348011016846 ------val\n",
      "epoch: 2160 loss: 8.303448562622071 -----------train\n",
      "epoch: 2160 val loss: 8.404295921325684 ------val\n",
      "epoch: 2220 loss: 8.295388565063476 -----------train\n",
      "epoch: 2220 val loss: 8.3969357808431 ------val\n",
      "epoch: 2280 loss: 8.29048698425293 -----------train\n",
      "epoch: 2280 val loss: 8.390214284261068 ------val\n",
      "epoch: 2340 loss: 8.28409797668457 -----------train\n",
      "epoch: 2340 val loss: 8.383556842803955 ------val\n",
      "epoch: 2400 loss: 8.2772163772583 -----------train\n",
      "epoch: 2400 val loss: 8.382997512817383 ------val\n",
      "epoch: 2460 loss: 8.270757484436036 -----------train\n",
      "epoch: 2460 val loss: 8.37592601776123 ------val\n",
      "epoch: 2520 loss: 8.26546443939209 -----------train\n",
      "epoch: 2520 val loss: 8.369102636973063 ------val\n",
      "epoch: 2580 loss: 8.259769821166993 -----------train\n",
      "epoch: 2580 val loss: 8.365328470865885 ------val\n",
      "epoch: 2640 loss: 8.253905334472655 -----------train\n",
      "epoch: 2640 val loss: 8.361841519673666 ------val\n",
      "epoch: 2700 loss: 8.249215354919434 -----------train\n",
      "epoch: 2700 val loss: 8.354686578114828 ------val\n",
      "epoch: 2760 loss: 8.243488311767578 -----------train\n",
      "epoch: 2760 val loss: 8.351954460144043 ------val\n",
      "epoch: 2820 loss: 8.2387105178833 -----------train\n",
      "epoch: 2820 val loss: 8.346296628316244 ------val\n",
      "epoch: 2880 loss: 8.233832092285157 -----------train\n",
      "epoch: 2880 val loss: 8.34336233139038 ------val\n",
      "epoch: 2940 loss: 8.229486770629883 -----------train\n",
      "epoch: 2940 val loss: 8.339980284372965 ------val\n",
      "epoch: 3000 loss: 8.224661903381348 -----------train\n",
      "epoch: 3000 val loss: 8.33522399266561 ------val\n",
      "epoch: 3060 loss: 8.221182861328124 -----------train\n",
      "epoch: 3060 val loss: 8.332027435302734 ------val\n",
      "epoch: 3120 loss: 8.21597053527832 -----------train\n",
      "epoch: 3120 val loss: 8.330458323160807 ------val\n",
      "epoch: 3180 loss: 8.212064094543457 -----------train\n",
      "epoch: 3180 val loss: 8.324179649353027 ------val\n",
      "epoch: 3240 loss: 8.209012031555176 -----------train\n",
      "epoch: 3240 val loss: 8.323150475819906 ------val\n",
      "epoch: 3300 loss: 8.204248123168945 -----------train\n",
      "epoch: 3300 val loss: 8.316673437754313 ------val\n",
      "epoch: 3360 loss: 8.199973869323731 -----------train\n",
      "epoch: 3360 val loss: 8.313949426015219 ------val\n",
      "epoch: 3420 loss: 8.196577835083009 -----------train\n",
      "epoch: 3420 val loss: 8.312813758850098 ------val\n",
      "epoch: 3480 loss: 8.194637565612792 -----------train\n",
      "epoch: 3480 val loss: 8.31056515375773 ------val\n",
      "epoch: 3540 loss: 8.189492225646973 -----------train\n",
      "epoch: 3540 val loss: 8.30758253733317 ------val\n",
      "epoch: 3600 loss: 8.18656940460205 -----------train\n",
      "epoch: 3600 val loss: 8.30631685256958 ------val\n",
      "epoch: 3660 loss: 8.182977333068848 -----------train\n",
      "epoch: 3660 val loss: 8.30127509435018 ------val\n",
      "epoch: 3720 loss: 8.180356941223145 -----------train\n",
      "epoch: 3720 val loss: 8.29875898361206 ------val\n",
      "epoch: 3780 loss: 8.17662899017334 -----------train\n",
      "epoch: 3780 val loss: 8.29534943898519 ------val\n",
      "epoch: 3840 loss: 8.172833595275879 -----------train\n",
      "epoch: 3840 val loss: 8.29118283589681 ------val\n",
      "epoch: 3900 loss: 8.170411338806153 -----------train\n",
      "epoch: 3900 val loss: 8.291781584421793 ------val\n",
      "epoch: 3960 loss: 8.167934265136719 -----------train\n",
      "epoch: 3960 val loss: 8.285825411478678 ------val\n",
      "epoch: 4020 loss: 8.165396461486816 -----------train\n",
      "epoch: 4020 val loss: 8.283898512522379 ------val\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writter = SummaryWriter(log_dir='./tf-logs') \n",
    "\n",
    "def training_loop(config, model, liu_gpt, optimizer, loss_fn, device, epochs, win_len, batch_size):\n",
    "    loss_list = []\n",
    "    loss_list_val = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        i_num = 0# 记录循环次数\n",
    "        loss_sum = 0\n",
    "        loss_sum_val = 0\n",
    "        # 训练\n",
    "        model.train()\n",
    "        for train_x, train_y in liu_gpt.seq_data(data=\"train_data\", win_len=win_len, batch_size=batch_size):\n",
    "            train_x = torch.tensor(train_x).to(device=device)\n",
    "            train_y = torch.tensor(train_y).to(device=device)\n",
    "            x = model(train_x, train_y)\n",
    "            loss = loss_fn(x, train_y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "            i_num += 1\n",
    "        loss_mean = loss_sum / i_num\n",
    "        if epoch%60 == 0:\n",
    "            print(\"epoch:\", epoch, \"loss:\", loss_mean, \"-----------train\")\n",
    "            loss_list.append(loss_mean)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            i_num_val = 0\n",
    "            for test_x, test_y in liu_gpt.seq_data(data=\"val_data\", win_len=win_len, batch_size=batch_size):\n",
    "                test_x = torch.tensor(test_x).to(device=device)\n",
    "                test_y = torch.tensor(test_y).to(device=device)\n",
    "                x = model(test_x, test_y)\n",
    "                loss_val = loss_fn(x, test_y.view(-1))\n",
    "                loss_sum_val += loss_val.item()\n",
    "                i_num_val += 1\n",
    "            loss_mean_val = loss_sum_val / i_num_val\n",
    "            if epoch%60 == 0:\n",
    "                print(\"epoch:\", epoch, \"val loss:\", loss_mean_val, \"------val\")\n",
    "                loss_list_val.append(loss_mean_val)\n",
    "        writter.add_scalars(\"train_val_loss\", {\"train_loss\": loss_mean, \"val_loss\": loss_mean_val}, epoch)\n",
    "    return loss_list, loss_list_val\n",
    "    \n",
    "            \n",
    "        \n",
    "\n",
    "def main():\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))  # GPU设备\n",
    "    config = GPTconfig()\n",
    "    model = GPTmodel(config).to(device=device)\n",
    "    liu_gpt = GPTDataset(\"my_bpe_model.model\", \"big_liu.txt\")\n",
    "\n",
    "    epochs = 12000\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    win_len = 128\n",
    "    batch_size = 300\n",
    "    loss_list, loss_list_val = training_loop(config, model, liu_gpt, optimizer, loss_fn, device, epochs, win_len, batch_size)\n",
    "\n",
    "    model.cpu()\n",
    "    torch.save(model.state_dict(), \"my_gpt_model.pt\")\n",
    "    torch.save(model, \"my_gpt_model_net.pth\")\n",
    "\n",
    "    with open(\"loss_list.csv\", \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for i in range(len(loss_list)):\n",
    "            writer.writerow([loss_list[i], loss_list_val[i]])\n",
    "    print(\"数据写入:loss_list\")    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e1b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
